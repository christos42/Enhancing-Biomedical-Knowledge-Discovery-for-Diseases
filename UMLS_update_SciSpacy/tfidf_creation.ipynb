{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59426712",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "###### SciSpacy repo: \n",
    "- https://github.com/allenai/scispacy/blob/main/scripts/create_linker.py\n",
    "- https://github.com/allenai/scispacy/blob/4f9ba0931d216ddfb9a8f01334d76cfb662738ae/scispacy/candidate_generation.py#L365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy\n",
    "import scipy\n",
    "import joblib\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, NamedTuple, Optional, Set, Tuple, Union, IO, Type\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from hashlib import sha256\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nmslib\n",
    "from nmslib.dist import FloatIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE = 'foo/'\n",
    "\n",
    "def cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path),\n",
    "    determine which. If it's a URL, download the file and cache it, and\n",
    "    return the path to the cached file. If it's already a local path,\n",
    "    make sure the file exists and then return the path.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = DATASET_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "\n",
    "    parsed = urlparse(url_or_filename)\n",
    "\n",
    "    if parsed.scheme in (\"http\", \"https\"):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        return get_from_cache(url_or_filename, cache_dir)\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        return url_or_filename\n",
    "    elif parsed.scheme == \"\":\n",
    "        # File, but it doesn't exist.\n",
    "        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\n",
    "            \"unable to parse {} as a URL or as a local path\".format(url_or_filename)\n",
    "        )\n",
    "\n",
    "\n",
    "def url_to_filename(url: str, etag: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert `url` into a hashed filename in a repeatable way.\n",
    "    If `etag` is specified, append its hash to the url's, delimited\n",
    "    by a period.\n",
    "    \"\"\"\n",
    "\n",
    "    last_part = url.split(\"/\")[-1]\n",
    "    url_bytes = url.encode(\"utf-8\")\n",
    "    url_hash = sha256(url_bytes)\n",
    "    filename = url_hash.hexdigest()\n",
    "\n",
    "    if etag:\n",
    "        etag_bytes = etag.encode(\"utf-8\")\n",
    "        etag_hash = sha256(etag_bytes)\n",
    "        filename += \".\" + etag_hash.hexdigest()\n",
    "\n",
    "    filename += \".\" + last_part\n",
    "    return filename\n",
    "\n",
    "\n",
    "def filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
    "    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = DATASET_CACHE\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise FileNotFoundError(\"file {} not found\".format(cache_path))\n",
    "\n",
    "    meta_path = cache_path + \".json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(\"file {} not found\".format(meta_path))\n",
    "\n",
    "    with open(meta_path) as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    url = metadata[\"url\"]\n",
    "    etag = metadata[\"etag\"]\n",
    "\n",
    "    return url, etag\n",
    "\n",
    "\n",
    "def http_get(url: str, temp_file: IO) -> None:\n",
    "    req = requests.get(url, stream=True)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            temp_file.write(chunk)\n",
    "\n",
    "\n",
    "def get_from_cache(url: str, cache_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Given a URL, look for the corresponding dataset in the local cache.\n",
    "    If it's not there, download it. Then return the path to the cached file.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = DATASET_CACHE\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.head(url, allow_redirects=True)\n",
    "    if response.status_code != 200:\n",
    "        raise IOError(\n",
    "            \"HEAD request failed for url {} with status code {}\".format(\n",
    "                url, response.status_code\n",
    "            )\n",
    "        )\n",
    "    etag = response.headers.get(\"ETag\")\n",
    "\n",
    "    filename = url_to_filename(url, etag)\n",
    "\n",
    "    # get cache path to put the file\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "\n",
    "    if not os.path.exists(cache_path):\n",
    "        # Download to temporary file, then copy to cache dir once finished.\n",
    "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:  # type: IO\n",
    "            print(f\"{url} not found in cache, downloading to {temp_file.name}\")\n",
    "\n",
    "            # GET file object\n",
    "            http_get(url, temp_file)\n",
    "\n",
    "            # we are copying the file before closing it, so flush to avoid truncation\n",
    "            temp_file.flush()\n",
    "            # shutil.copyfileobj() starts at the current position, so go to the start\n",
    "            temp_file.seek(0)\n",
    "\n",
    "            print(\n",
    "                f\"Finished download, copying {temp_file.name} to cache at {cache_path}\"\n",
    "            )\n",
    "            with open(cache_path, \"wb\") as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "\n",
    "            meta = {\"url\": url, \"etag\": etag}\n",
    "            meta_path = cache_path + \".json\"\n",
    "            with open(meta_path, \"w\") as meta_file:\n",
    "                json.dump(meta, meta_file)\n",
    "\n",
    "    return cache_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(NamedTuple):\n",
    "\n",
    "    concept_id: str\n",
    "    canonical_name: str\n",
    "    aliases: List[str]\n",
    "    types: List[str] = []\n",
    "    definition: Optional[str] = None\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        rep = \"\"\n",
    "        num_aliases = len(self.aliases)\n",
    "        rep = rep + f\"CUI: {self.concept_id}, Name: {self.canonical_name}\\n\"\n",
    "        rep = rep + f\"Definition: {self.definition}\\n\"\n",
    "        rep = rep + f\"TUI(s): {', '.join(self.types)}\\n\"\n",
    "        if num_aliases > 10:\n",
    "            rep = (\n",
    "                rep\n",
    "                + f\"Aliases (abbreviated, total: {num_aliases}): \\n\\t {', '.join(self.aliases[:10])}\"\n",
    "            )\n",
    "        else:\n",
    "            rep = (\n",
    "                rep + f\"Aliases: (total: {num_aliases}): \\n\\t {', '.join(self.aliases)}\"\n",
    "            )\n",
    "        return rep\n",
    "\n",
    "\n",
    "DEFAULT_UMLS_PATH = \"https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/kbs/2020-10-09/umls_2020_aa_cat0129.jsonl\"  # noqa\n",
    "DEFAULT_UMLS_TYPES_PATH = \"https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/umls_semantic_type_tree.tsv\"\n",
    "#DEFAULT_UMLS_TYPES_PATH = \"2022AB/umls_semantic_type_tree.tsv\"\n",
    "\n",
    "\n",
    "class KnowledgeBase:\n",
    "    \"\"\"\n",
    "    A class representing two commonly needed views of a Knowledge Base:\n",
    "    1. A mapping from concept_id to an Entity NamedTuple with more information.\n",
    "    2. A mapping from aliases to the sets of concept ids for which they are aliases.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str, required.\n",
    "        The file path to the json/jsonl representation of the KB to load.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str = None,\n",
    "    ):\n",
    "        if file_path is None:\n",
    "            raise ValueError(\n",
    "                \"Do not use the default arguments to KnowledgeBase. \"\n",
    "                \"Instead, use a subclass (e.g UmlsKnowledgeBase) or pass a path to a kb.\"\n",
    "            )\n",
    "        if file_path.endswith(\"jsonl\"):\n",
    "            raw = (json.loads(line) for line in open(cached_path(file_path)))\n",
    "        else:\n",
    "            raw = json.load(open(cached_path(file_path)))\n",
    "\n",
    "        alias_to_cuis: Dict[str, Set[str]] = defaultdict(set)\n",
    "        self.cui_to_entity: Dict[str, Entity] = {}\n",
    "\n",
    "        for concept in raw:\n",
    "            unique_aliases = set(concept[\"aliases\"])\n",
    "            unique_aliases.add(concept[\"canonical_name\"])\n",
    "            for alias in unique_aliases:\n",
    "                alias_to_cuis[alias].add(concept[\"concept_id\"])\n",
    "            self.cui_to_entity[concept[\"concept_id\"]] = Entity(**concept)\n",
    "\n",
    "        self.alias_to_cuis: Dict[str, Set[str]] = {**alias_to_cuis}\n",
    "\n",
    "\n",
    "class UmlsKnowledgeBase(KnowledgeBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str = DEFAULT_UMLS_PATH,\n",
    "        types_file_path: str = DEFAULT_UMLS_TYPES_PATH,\n",
    "    ):\n",
    "\n",
    "        super().__init__(file_path)\n",
    "\n",
    "        self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(\n",
    "            types_file_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e67b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_ann_index(out_path: str, kb: KnowledgeBase = None) -> Tuple[List[str], TfidfVectorizer, FloatIndex]:\n",
    "    \"\"\"\n",
    "    Build tfidf vectorizer and ann index.\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_path: str, required.\n",
    "        The path where the various model pieces will be saved.\n",
    "    kb : KnowledgeBase, optional.\n",
    "        The kb items to generate the index and vectors for.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer_path = f\"{out_path}/tfidf_vectorizer.joblib\"\n",
    "    ann_index_path = f\"{out_path}/nmslib_index.bin\"\n",
    "    tfidf_vectors_path = f\"{out_path}/tfidf_vectors_sparse.npz\"\n",
    "    uml_concept_aliases_path = f\"{out_path}/concept_aliases.json\"\n",
    "\n",
    "    kb = kb or UmlsKnowledgeBase()\n",
    "\n",
    "    # nmslib hyperparameters (very important)\n",
    "    # guide: https://github.com/nmslib/nmslib/blob/master/python_bindings/parameters.md\n",
    "    # Default values resulted in very low recall.\n",
    "\n",
    "    # set to the maximum recommended value. Improves recall at the expense of longer indexing time.\n",
    "    # We use the HNSW (Hierarchical Navigable Small World Graph) representation which is constructed\n",
    "    # by consecutive insertion of elements in a random order by connecting them to M closest neighbours\n",
    "    # from the previously inserted elements. These later become bridges between the network hubs that\n",
    "    # improve overall graph connectivity. (bigger M -> higher recall, slower creation)\n",
    "    # For more details see:  https://arxiv.org/pdf/1603.09320.pdf?\n",
    "    m_parameter = 100\n",
    "    # `C` for Construction. Set to the maximum recommended value\n",
    "    # Improves recall at the expense of longer indexing time\n",
    "    construction = 2000\n",
    "    num_threads = 60  # set based on the machine\n",
    "    index_params = {\n",
    "        \"M\": m_parameter,\n",
    "        \"indexThreadQty\": num_threads,\n",
    "        \"efConstruction\": construction,\n",
    "        \"post\": 0,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"No tfidf vectorizer on {tfidf_vectorizer_path} or ann index on {ann_index_path}\"\n",
    "    )\n",
    "    concept_aliases = list(kb.alias_to_cuis.keys())\n",
    "\n",
    "    # NOTE: here we are creating the tf-idf vectorizer with float32 type, but we can serialize the\n",
    "    # resulting vectors using float16, meaning they take up half the memory on disk. Unfortunately\n",
    "    # we can't use the float16 format to actually run the vectorizer, because of this bug in sparse\n",
    "    # matrix representations in scipy: https://github.com/scipy/scipy/issues/7408\n",
    "    print(f\"Fitting tfidf vectorizer on {len(concept_aliases)} aliases\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\", ngram_range=(3, 3), min_df=10, dtype=numpy.float32\n",
    "    )\n",
    "    start_time = datetime.datetime.now()\n",
    "    concept_alias_tfidfs = tfidf_vectorizer.fit_transform(concept_aliases)\n",
    "    print(f\"Saving tfidf vectorizer to {tfidf_vectorizer_path}\")\n",
    "    joblib.dump(tfidf_vectorizer, tfidf_vectorizer_path)\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Fitting and saving vectorizer took {total_time.total_seconds()} seconds\")\n",
    "\n",
    "    print(\"Finding empty (all zeros) tfidf vectors\")\n",
    "    empty_tfidfs_boolean_flags = numpy.array(\n",
    "        concept_alias_tfidfs.sum(axis=1) != 0\n",
    "    ).reshape(-1)\n",
    "    number_of_non_empty_tfidfs = sum(empty_tfidfs_boolean_flags == False)  # noqa: E712\n",
    "    total_number_of_tfidfs = numpy.size(concept_alias_tfidfs, 0)\n",
    "\n",
    "    print(\n",
    "        f\"Deleting {number_of_non_empty_tfidfs}/{total_number_of_tfidfs} aliases because their tfidf is empty\"\n",
    "    )\n",
    "    # remove empty tfidf vectors, otherwise nmslib will crash\n",
    "    concept_aliases = [\n",
    "        alias\n",
    "        for alias, flag in zip(concept_aliases, empty_tfidfs_boolean_flags)\n",
    "        if flag\n",
    "    ]\n",
    "    concept_alias_tfidfs = concept_alias_tfidfs[empty_tfidfs_boolean_flags]\n",
    "    assert len(concept_aliases) == numpy.size(concept_alias_tfidfs, 0)\n",
    "\n",
    "    print(\n",
    "        f\"Saving list of concept ids and tfidfs vectors to {uml_concept_aliases_path} and {tfidf_vectors_path}\"\n",
    "    )\n",
    "    json.dump(concept_aliases, open(uml_concept_aliases_path, \"w\"))\n",
    "    scipy.sparse.save_npz(\n",
    "        tfidf_vectors_path, concept_alias_tfidfs.astype(numpy.float16)\n",
    "    )\n",
    "\n",
    "    print(f\"Fitting ann index on {len(concept_aliases)} aliases\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    ann_index = nmslib.init(\n",
    "        method=\"hnsw\",\n",
    "        space=\"cosinesimil_sparse\",\n",
    "        data_type=nmslib.DataType.SPARSE_VECTOR,\n",
    "    )\n",
    "    ann_index.addDataPointBatch(concept_alias_tfidfs)\n",
    "    ann_index.createIndex(index_params, print_progress=True)\n",
    "    ann_index.saveIndex(ann_index_path)\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Fitting ann index took {elapsed_time.total_seconds()} seconds\")\n",
    "\n",
    "    return concept_aliases, tfidf_vectorizer, ann_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617de881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(kb_path: str, output_path: str):\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    kb = KnowledgeBase(kb_path)\n",
    "    create_tfidf_ann_index(output_path, kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/drugbank_umls_2022_ab.jsonl', '2022AB/linkers/drugbank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/go_umls_2022_ab.jsonl', '2022AB/linkers/go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/gs_umls_2022_ab.jsonl', '2022AB/linkers/gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/hpo_umls_2022_ab.jsonl', '2022AB/linkers/hpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37807349",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/mesh_umls_2022_ab.jsonl', '2022AB/linkers/mesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/ncbi_umls_2022_ab.jsonl', '2022AB/linkers/ncbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/rxnorm_umls_2022_ab.jsonl', '2022AB/linkers/rxnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/snomed_umls_2022_ab.jsonl', '2022AB/linkers/snomed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0049f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('2022AB/umls_2022_ab.jsonl', '2022AB/linkers/umls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
